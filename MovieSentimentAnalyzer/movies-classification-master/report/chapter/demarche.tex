\chapter{Démarche}
%---------------------------------------------------------------------------------------------------------------
\section{Liste des films}

Tout d'abord, nous avons trouvé une liste de films qui provient du site \url{IMDb.com}. Elle contient des informations sur 3393 films connus. Cela nous permet d'avoir une liste fixe et de ne pas avoir besoin d'aller les chercher à chaque fois sur internet. Nous avons donc stocké ce fichier que nous utiliserons par la suite. Ce fichier est formaté de la manière suivante : une ligne correspond à un film, chaque ligne contient 5 informations sur le film : \\

\begin{itemize}
 \item Le titre du film
 \item L'année de sortie du film
 \item La note qu'à le film sur IMDb
 \item Le nombre de vote qu'à reçu le film sur IMDb
 \item Le genre (plusieurs genres séparés par des virgules)
\end{itemize}

\vspace{0.4cm}
Chacune des ces cinq informations est séparée par deux espaces. Voici un aperçu de cette structure de fichier : \\

\begin{lstlisting}[language=bash]
  The Pianist﻿  2002﻿  8.5﻿  151864﻿  Biography,Drama,War
  The Godfather﻿  1972﻿  9.2﻿  450433﻿  Crime,Drama,Thriller
  Casper﻿  1995﻿  5.7﻿  28316﻿  Comedy,Family,Fantasy
  Hostel﻿  2005﻿  5.7﻿  69499﻿  Mystery,Thriller
  Juno﻿  2007﻿  7.8﻿  163142﻿  Comedy,Drama,Romance
\end{lstlisting}

Cette structure nous permet donc d'extraire facilement les informations voulues avec la fonction Python suivante : \\

\begin{lstlisting}[language=python]
  def extractMovieList(fname):

      with open(fname) as f:
          content = f.readlines()
    
      title = []
      year = []
      grade = []
      votes = []
      genre = []

      for line in content:
          infos = line.split('  ')
          title.append(infos[0].replace('"',''))
          year.append(infos[1])
          grade.append(infos[2])
          votes.append(infos[3])
          genre.append(infos[4])

      return [title, year, grade, genre]
\end{lstlisting}

%---------------------------------------------------------------------------------------------------------------
\section{Recherche d'information sur un film}

Les données disponibles dans le fichier \texttt{movielist.txt} ne nous permetteront pas de classifier les films. C'est juste une liste qui nous permet d'avoir une quantité de titre de films ainsi que les genres de chaque film. Nous voulons alors rechercher les descriptifs de chaque film. Pour cela, nous utilisons une API\footnote{\url{http://fr.wikipedia.org/wiki/Interface\_de\_programmation}} d'IMDb qui est documentée à l'adresse \url{http://omdbapi.com}. Cette API nous permet de chercher des informations sur un film soit en donnant le titre, soit en donnant l'identitiant IMDb du film (ce que nous n'avons pas). 

Nous allons alors prendre la liste de films et faire une requête sur cette API pour chaque titre de film que nous avons. De temps en temps, aucun résultat ne sort, nous passons simplement au suivant. Nous enregistrons alors tous les résultats dans un fichier sous format JSON.\footnote{\url{http://fr.wikipedia.org/wiki/JavaScript\_Object\_Notation}} \\

\begin{lstlisting}[language=python]
  def getMovieDescriptions(movieTitles, fname):

      movieInfos = []
      base_url = "http://www.omdbapi.com/?plot=full&r=json&t="
    
      for title in movieTitles:
          print 'fetching ' + title
          url = base_url + title
          r = requests.get(url)
          movieInfos.append(r.json())

      with open(fname, 'wb') as outfile:
          json.dump(movieInfos, outfile)
\end{lstlisting}

Pour chaque film, nous recevons différentes informations, dont un lien vers une image, le titre, la durée du film, l'année, les acteurs, le pays, la langue, l'identifiant IMDb, un descriptif, etc. Ce qui nous intéresse à première vue est le descriptif. Seulement après avoir analysé ces descriptifs, nous remarquons qu'ils sont souvent courts, ils comportent pour la plupart moins de cinquante mots. Cela est beaucoup trop peu pour pouvoir trouver des ressemblances entres les films et les classifier. 

Nous avons alors eu l'idée d'utiliser l'identifiant IMDb de chaque films pour aller chercher plus d'informations sur les films. Après quelques recherches, nous trouvons qu'il y a une page spécifique pour chaque film sur le site IMDb où se trouve un résumé plus détaillé du film. L'adresse est formée de la façon suivante : \textit{'http://www.imdb.com/title/[imdbId]/synopsis'}. Il nous reste donc à remplacer l'identifiant que l'on veut dans cette URL et nous arrivons sur un beau descriptif, qu'il faut tout de même extraire de la page web (parser du HTML est toujours une partie de plaisir, quoique).

Un script Python va nous permettre d'aller chercher le descriptif de chaque film et de l'enregistrer cette fois dans un fichier séparé pour chaque film. Voici ce script : \\

\lstinputlisting[language=Python]{code/fetchSynopsis.py}

Tous les films n'avaient pas leur page de descriptif correspondante. Sur les 3393 films initiaux, seuls 2396 ont pu être extrait et enregistré. C'est tout de même assez pour faire nos tests. Nous continuons donc avec cela.

%---------------------------------------------------------------------------------------------------------------
\section{Création du dataset}

Nous avons maintenant tout en main, et en local, pour pouvoir générer les structures de données spécifiques. Nous avons besoin de trois structures différentes :

\begin{itemize}
 \item un vecteur contenant la liste des films
 \item un vecteur contenant la liste de tous les mots qui apparaissent dans les textes de description 
 \item une matrix contenant le nombre d'occurrence de chaque mot pour chaque film. Chaque ligne correspond à un film et chaque colonne à un mot.
\end{itemize}

\vspace{0.4cm}
%\lstinputlisting[language=Python]{code/formatData.py}
Voici comment la liste des titres de film est générée : \\

\begin{lstlisting}[language=python]
  def formatTitle(data):
      print 'Generating title list...'
      titles = []
      i = 0;
      for movie in data:
          fname = 'data2/synopsis/' + movie['imdbID'] + '.txt'
          # if synopsis file exists
          if os.path.isfile(fname):
              titles.append(movie['Title'])
              i = i + 1
      print 'There is ' + str(i) + ' movies.'
      return titles
\end{lstlisting}

Il a fallu ensuite générer la liste de tous les mots qui apparaissent dans les descriptifs de chaque film. Là il fallait faire plusieurs traitement pour enlever la ponctuation et surtout enlever les \textit{``stop words''} (ou \textit{``mots vides''} en français) qui sont les mots qui n'ont pas vraiment de valeur comme les déterminants, les pronoms. Ces mots apparaissent avec une fréquence contante dans les différents textes et n'ont donc pas de valeur dans la distinction entre les textes. Pour ce faire, nous avons utilisé la librairie python \texttt{nltk}. Nous avons ensuite fait une liste des mots uniques restants. Voici le code correspondant : \\

\begin{lstlisting}[language=python]
  def formatWords(data):
      print 'Generating word list...'
      words = []
      for movie in data:
          fname = 'data2/synopsis/' + movie['imdbID'] + '.txt'
          # if synopsis file exists
          if os.path.isfile(fname):
              synopsis = ''
              with open(fname, 'r') as thefile:
                  synopsis = thefile.read().replace('\n', '')

              synopsis = synopsis.lower()
              # clean words
              word_list = re.findall(r'\w+', synopsis, flags = re.UNICODE | re.LOCALE) 
              # remove the stopwords
              word_list = filter(lambda x: x not in stopwords.words('english'), word_list)
              for word in word_list:
                  #word = clean(word)
                  if word not in words:
                      words.append(word)
      print 'There is ', len(words), ' words in the list.'
      return words
\end{lstlisting}

La partie la plus difficile est de générer la matrice indiquant combien de fois chaque mot apparaît dans chaque texte. Il a fallu optimiser le maximum ce traitement car il prend énormément de temps à l'exécution. Et voici donc comment la matrice est générée : \\

\begin{lstlisting}[language=python]
  def generateMatrix(data, words):
      print 'Generating matrix...'
      matrix = []
      i = 0
    
      for movie in data:
          i = i + 1
          fname = 'data2/synopsis/' + movie['imdbID'] + '.txt'
          # if synopsis file exists
          if os.path.isfile(fname):
              print 'movie ' + str(i) + ' on ' + str(len(data))
              line = []
              synopsis = ''
              with open(fname, 'r') as thefile:
                  synopsis = thefile.read().replace('\n', '')

              synopsis = synopsis.lower()
              word_list = re.findall(r'\w+', synopsis, flags = re.UNICODE | re.LOCALE) 
              word_list = filter(lambda x: x not in stopwords.words('english'), word_list)
              for word in words:
                  count = 0

                  for w in word_list:
                      if w == word:
                          count = count + 1
                  line.append(count)

              matrix.append(line)
          else :
              print 'movie ' + str(i) + ' has no synopsis'
      print 'Done.'
      return matrix
\end{lstlisting}

Avec ces trois matrices, nous pouvons directement faire marcher les Self-Organizing Maps, mais après plusieurs essais, nous nous sommes rendu compte que les résultats n'étaient pas très parlant.

%---------------------------------------------------------------------------------------------------------------
\section{Optimisations}

Une première amélioration a été de supprimer les mots qui n'apparaissent que dans un seul film. En effet, ces mots n'ont pas de valeur (au même titre que les stopwords) car ils ne permettent pas de faire des corrélations entre différents films. Nous avons donc fait un premier traitement qui supprime ces mots. Il a donc fallu supprimer ces mots de la liste des mots, mais étalement supprimer toutes les colonnes de la matrice correspondantes à ces mots. Grâce à la librairie \texttt{Numpy}, ce traitement est assez peu gourmand en nombre de lignes de code. Nous utilisons un masque pour filtrer les résultats. \\

\begin{lstlisting}[language=python]
  def removeLonelyWords(words, matrix):

      word_counts = matrix.sum(axis=0)
      word_mask = word_counts > 1

      # convert words to numpy array
      words = np.array(words)

      matrix = matrix[:,word_mask]
      words = words[word_mask]

      return words.tolist(), matrix
\end{lstlisting}

Les résultats générés avec ces matrices n'était toujours pas suffisant pour classifier de manière optimale les films. Nous avons alors appliqué l'algorithme TF-IDF\footnote{\url{http://fr.wikipedia.org/wiki/TF-IDF}} (\textit{Term Frequency-Inverse Document Frequency}) à la matrice. Cet algorithme permet d'évaluer l'importance d'un mot contenu dans un texte, relativement à une série de textes. Voici les formules correspondantes :

\[idf_i = log\frac{N}{t_j : w_i \in t_j}\]
\[tfidf_{ij} = tf_{ij} \cdot idf_i\]

Le terme $tf_{ij}$ est la matrice avec la fréquence de mots dans chaque texte que nous avons déjà générée. $N$ est le nombre total de textes. $t_j$ est le nombre de textes où le mot $w_i$ apparaît. $tfidf_{ij}$ est la matrice finale, obtenue avec l'algorithme TF-IDF. Nous allons utiliser cette matrice en remplacement de la précédente pour générer la carte de Kohonen. Voici le code qui permet cette optimisation : \\

\begin{lstlisting}[language=python] 
 def applyTFIDF(matrix):

      tf = matrix
      N = matrix.shape[0]
      matrix_mask = matrix > 0
      nf = matrix_mask.sum(axis=0)
      idf = np.log10(nf*(1.0/N))
      tfidf = tf * idf

      return tfidf
\end{lstlisting}

Lors de la sauvegarde du fichier contenant les deux vecteurs et la matrice, nous passons d'un fichier de 287.7Mo à un fichier de 581.7Mo. Il y a au total 39404 mots différents pour 2396 films. Cela est dû au fait que les valeurs de la matrice sont des nombres à virgules à la place de simples entiers.
\newpage
%---------------------------------------------------------------------------------------------------------------
\section{Matrice de distances}

\label{matrice-distance}
Pour avoir une notion de distance entre les documents nous avons créé une matrice des distances. Pour faire cela, on a utilisé la librairie "hcluster" \footnote{Hcluster, \url{https://code.google.com/p/scipy-cluster/}} qui prend en entrée une matrice de vecteurs des documents et elle permet de définir une métrique de distance : dans notre cas on a utilisé la fonction "cosine".

Le code pour la génération de la matrice de distance est le suivant : \\

\begin{lstlisting}[language=python]
  titles, words, matrix = extractArrays(infos)
  distanceMatrix = pairwise_distances(matrix, metric='cosine')
\end{lstlisting}


%---------------------------------------------------------------------------------------------------------------
\section{Liste des films similaires}

Pour voir si la matrice des vecteurs des documents décrit bien les documents nous avons implémenté une fonction qui, avec un film en paramètre, retourne les N films las plus similaires à celui-ci.

La fonction prend en paramètre l'id du film initial, le nombre des films similaires à afficher, la matrice de distance décrite dans la section \ref{matrice-distance} ainsi que le vecteur des titres des films.

Le code de cette fonction est le suivant : \\

\begin{lstlisting}[language=python]
  def printClosest(idxFilm, numclosest, distanceMatrix, titles):
    print titles[idxFilm]+":"
    cloasest= heapq.nsmallest(numclosest,range(len(distanceMatrix[idxFilm])),distanceMatrix[idxFilm].take)
    for idx, val in enumerate(cloasest):
        print  "\t"+str(idx)+" "+titles[val]
\end{lstlisting}


%---------------------------------------------------------------------------------------------------------------
\section{Clustering hiérarchique}

Une fois avoir évalué quelques films avec la liste des films similaires on a voulu vérifier mieux cela en faisan un clastering hiérarchique en vérifiant que les filmes regroupé dans un clastering sont une catégorie similaire.

Pour faire cela, on utilise la libraire "hcluster" pour effectuer le clustering hiérarchique en se basant sur la matrice de distances.

Le clustering hiérarchique sera visualisé dans un dendrogram représentant les regroupements des films. \\

\begin{lstlisting}[language=python]
  Z=linkage(distanceMatrix,method='average')#,method='centroid')
  print Z.shape
  image=dendrogram(Z,labels=titlesCat, distance_sort='descendent',
         leaf_font_size=2, orientation='left', show_contracted=False)
  pylab.savefig("images/clustering100_tf_idf.png",dpi=300,bbox_inches='tight')	 	  
\end{lstlisting}

Pour nous aider à la visualisation du graphique on a utilisé une fonction utilitaire pour visionner les films les plus similaires que sont regroupé par l'algorithme de clustering.

Pour faire cela on a fait la fonction suivante : \\

\begin{lstlisting}[language=python]
  print "first closest cluster"
  for idx in range(10):
      lenTitle=len(titles)
      if (int(Z[idx,0])<lenTitle) & (int(Z[idx,1])<lenTitle):
          print "itr "+str(idx)+":\n"+titlesCat[int(Z[idx,0])]+" "+titlesCat[int(Z[idx,1])]
\end{lstlisting}

La variable Z a été calculée par la fonction de la libraire "hcluter" et on peut voir, à chaque itération, quel film a été mis ensemble avec quelle outre.	


%---------------------------------------------------------------------------------------------------------------
\section{Map de Kohonen}

Une fois avoir vu les résultats depuis le clustering hiérarchique on peut analyser la matrice initiale avec un algorithme un peu plus avancé et qui donne des résultats plus visuels.

On a utilisé l'algorithme de Khonen qui prend en entrée notre matrice des vecteurs des documents et produise en sortie une map en couleur avec la position de chaque filme par rapport a les autres. Les couleurs de la map représente la distance entre chaque film. 

Pour faire cela, on a utilisé la base du code fait dans le TP 
4 \footnote{TP Kohonen, HES-SO, \url{http://193.134.218.37/labs/lab4/lab4_assignment.html}}. Dans celui-là  on a modifié la métrique de mesure de la distance et la construction de la matrice initiale : \\

\begin{lstlisting}[language=python]
  # define cosine metric for configure distance metrix on kohonen
  def cosine_metric(x, y):
      # Returns the cosine distance between x and y.
      nx = np.sqrt(np.sum(x * x, axis=-1))
      ny = np.sqrt(np.sum(y * y, axis=-1))
      # the cosine metric returns 1 when the args are equal, 0 when they are
      # orthogonal, and -1 when they are opposite. we want the opposite effect,
      # and we want to make sure the results are always nonnegative.
      return 1 - np.sum(x * y, axis=-1) / nx / ny

  params = kohonen.Parameters(dimension=len(words), shape=(side,side*2), metric=cosine_metric)
  kmap = kohonen.Map(params)
\end{lstlisting}




%---------------------------------------------------------------------------------------------------------------
% Code integration example
%\begin{lstlisting}[language=bash]
%  sudo apt-get update
%  sudo apt-get install drupal7
%\end{lstlisting}

% Image integration example
%\begin{figure}[h]
%  \centering
%    \includegraphics[width=1\linewidth]{img/drupalFirstPage.png}
%  \caption{Page d'accueil du site créé avec Drupal sur une instance EC2}
%  \label{drupalfirstpage}
%\end{figure}

% Image side-by-side
%\begin{figure}[h!]
%    \centering
%    \begin{tabular}{cccc}
%      \includegraphics[width=.14\linewidth]{randomTree_n5.png} &
%      \includegraphics[width=.22\linewidth]{randomTree_n10.png} &
%      \includegraphics[width=.22\linewidth]{randomTree_n15.png} \\
%      (a) & (b) & (c)\\
%    \end{tabular}
%    \caption{Arbres aléatoires où (a) n=5 (b) n=10 (c) n=15
%    \label{randomTrees}}
%\end{figure}